{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Analyzing Edit Histories from Connectome Datasets\n",
    "\n",
    "This notebook demonstrates the complete pipeline for:\n",
    "1. Getting proofread neurons from MICrONS (mouse) dataset\n",
    "2. Fetching edit histories for those neurons\n",
    "3. Analyzing edit patterns (merge vs split, distributions)\n",
    "4. Understanding the data structure\n",
    "\n",
    "This is the core analysis for estimating computational proofreading costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caveclient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connect to MICrONS Dataset\n",
    "\n",
    "The MICrONS dataset (mouse cortex) is publicly accessible via CAVEclient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: minnie65_public\n",
      "Server: https://global.daf-apis.com\n"
     ]
    }
   ],
   "source": [
    "# Connect to the dataset\n",
    "species = \"mouse\"\n",
    "\n",
    "if species == \"mouse\":\n",
    "    client = caveclient.CAVEclient(\"minnie65_public\")\n",
    "elif species == \"human\":\n",
    "    client = caveclient.CAVEclient(\"h01_c3_flat\")\n",
    "elif species == \"fly\":\n",
    "    client = caveclient.CAVEclient(\"flywire_fafb_public\")\n",
    "elif species == \"zebrafish\":\n",
    "    client = caveclient.CAVEclient(\"fish1_full\")\n",
    "\n",
    "print(f\"Connected to: {client.datastack_name}\")\n",
    "print(f\"Server: {client.server_address}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables in materialize:\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "503 Server Error: Service Temporarily Unavailable for url: https://minnie.microns-daf.com/materialize/version content:b'<html>\\r\\n<head><title>503 Service Temporarily Unavailable</title></head>\\r\\n<body>\\r\\n<center><h1>503 Service Temporarily Unavailable</h1></center>\\r\\n<hr><center>nginx</center>\\r\\n</body>\\r\\n</html>\\r\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# get_tables() - returns a list of table names\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m tables = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaterialize\u001b[49m.get_tables()\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tables)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tables:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tables, \u001b[32m1\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConnectomeBench/.venv/lib/python3.12/site-packages/caveclient/frameworkclient.py:633\u001b[39m, in \u001b[36mCAVEclientFull.materialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[33;03mA client for the materialization service. See [client.materialize](../api/materialize.md)\u001b[39;00m\n\u001b[32m    630\u001b[39m \u001b[33;03mfor more information.\u001b[39;00m\n\u001b[32m    631\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._materialize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m     \u001b[38;5;28mself\u001b[39m._materialize = \u001b[43mMaterializationClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlocal_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth_client\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatastack_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_datastack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynapse_table\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_datastack_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msynapse_table\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_max_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_maxsize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool_maxsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_block\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mover_client\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesired_resolution\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesired_resolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._materialize\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConnectomeBench/.venv/lib/python3.12/site-packages/caveclient/materializationengine.py:221\u001b[39m, in \u001b[36mMaterializationClient.__init__\u001b[39m\u001b[34m(self, server_address, datastack_name, auth_client, cg_client, synapse_table, api_version, version, verify, max_retries, pool_maxsize, pool_block, desired_resolution, over_client)\u001b[39m\n\u001b[32m    209\u001b[39m auth_header = auth_client.request_header\n\u001b[32m    210\u001b[39m endpoints, api_version = _api_endpoints(\n\u001b[32m    211\u001b[39m     api_version,\n\u001b[32m    212\u001b[39m     SERVER_KEY,\n\u001b[32m   (...)\u001b[39m\u001b[32m    218\u001b[39m     verify=verify,\n\u001b[32m    219\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMaterializationClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mSERVER_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpool_maxsize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_maxsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpool_block\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mover_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mover_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28mself\u001b[39m._datastack_name = datastack_name\n\u001b[32m    234\u001b[39m \u001b[38;5;28mself\u001b[39m._version = version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConnectomeBench/.venv/lib/python3.12/site-packages/caveclient/base.py:217\u001b[39m, in \u001b[36mClientBase.__init__\u001b[39m\u001b[34m(self, server_address, auth_header, api_version, endpoints, server_name, verify, max_retries, pool_maxsize, pool_block, over_client)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m._endpoints = endpoints\n\u001b[32m    216\u001b[39m \u001b[38;5;28mself\u001b[39m._fc = over_client\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28mself\u001b[39m._server_version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConnectomeBench/.venv/lib/python3.12/site-packages/caveclient/base.py:246\u001b[39m, in \u001b[36mClientBase._get_version\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     version_str = \u001b[43mhandle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_json\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     version = Version(version_str)\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConnectomeBench/.venv/lib/python3.12/site-packages/caveclient/base.py:94\u001b[39m, in \u001b[36mhandle_response\u001b[39m\u001b[34m(response, as_json, log_warning)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Deal with potential errors in endpoint response and return json for default case\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# NOTE: consider adding \"None on 404\" as an option?\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43m_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_warning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_warning\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m _check_authorization_redirect(response)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m as_json:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ConnectomeBench/.venv/lib/python3.12/site-packages/caveclient/base.py:84\u001b[39m, in \u001b[36m_raise_for_status\u001b[39m\u001b[34m(r, log_warning)\u001b[39m\n\u001b[32m     76\u001b[39m     http_error_msg = \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m content:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (\n\u001b[32m     77\u001b[39m         r.status_code,\n\u001b[32m     78\u001b[39m         reason,\n\u001b[32m     79\u001b[39m         r.url,\n\u001b[32m     80\u001b[39m         r.content,\n\u001b[32m     81\u001b[39m     )\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m requests.HTTPError(http_error_msg, response=r)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m log_warning:\n\u001b[32m     86\u001b[39m     warning = r.headers.get(\u001b[33m\"\u001b[39m\u001b[33mWarning\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 503 Server Error: Service Temporarily Unavailable for url: https://minnie.microns-daf.com/materialize/version content:b'<html>\\r\\n<head><title>503 Service Temporarily Unavailable</title></head>\\r\\n<body>\\r\\n<center><h1>503 Service Temporarily Unavailable</h1></center>\\r\\n<hr><center>nginx</center>\\r\\n</body>\\r\\n</html>\\r\\n'"
     ]
    }
   ],
   "source": [
    "# List all available tables\n",
    "print(\"Available tables in materialize:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# get_tables() - returns a list of table names\n",
    "tables = client.materialize.get_tables()\n",
    "print(f\"Found {len(tables)} tables:\\n\")\n",
    "for i, table in enumerate(tables, 1):\n",
    "    print(f\"  {i}. {table}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get Proofread Neurons\n",
    "\n",
    "For mouse, the `<proofread_table>` is \"proofreading_status_and_strategy\".\n",
    "For human, the `<proofread_table>` is \"proofreading_status_test\".\n",
    "\n",
    "There are different ways to query neurons:\n",
    "<ul>\n",
    "<li>**`query_table('<proofread_table>')`**: Officially proofread neurons</li>\n",
    "<li>**`live_query('<proofread_table>', datetime.now())`**: Live version (Jeff's approach)</li>\n",
    "</ul>\n",
    "\n",
    "Let's explore both and understand the difference between `pt_root_id` and `valid_id` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying table: proofread_neurons\n",
      "Total records: 139,255\n",
      "\n",
      "Columns: ['id', 'created', 'superceded_id', 'valid', 'pt_supervoxel_id', 'pt_root_id', 'pt_position']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created</th>\n",
       "      <th>superceded_id</th>\n",
       "      <th>valid</th>\n",
       "      <th>pt_supervoxel_id</th>\n",
       "      <th>pt_root_id</th>\n",
       "      <th>pt_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32414</td>\n",
       "      <td>2023-06-19 06:44:57.863498+00:00</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "      <td>79871411195357919</td>\n",
       "      <td>720575940620919646</td>\n",
       "      <td>[538210, 198330, 105601]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1125</td>\n",
       "      <td>2023-06-19 06:43:33.633089+00:00</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "      <td>77195887289571552</td>\n",
       "      <td>720575940611775973</td>\n",
       "      <td>[381459, 106763, 171723]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32416</td>\n",
       "      <td>2023-06-19 06:44:57.865882+00:00</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "      <td>78955586729523534</td>\n",
       "      <td>720575940618135198</td>\n",
       "      <td>[486480, 133840, 123110]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32418</td>\n",
       "      <td>2023-06-19 06:44:57.867936+00:00</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "      <td>79589661206805006</td>\n",
       "      <td>720575940624783287</td>\n",
       "      <td>[523906, 181893, 72760]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32419</td>\n",
       "      <td>2023-06-19 06:44:57.869497+00:00</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "      <td>79589661207074646</td>\n",
       "      <td>720575940630755276</td>\n",
       "      <td>[523523, 181919, 80829]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                          created  superceded_id  valid  \\\n",
       "0  32414 2023-06-19 06:44:57.863498+00:00           <NA>   True   \n",
       "1   1125 2023-06-19 06:43:33.633089+00:00           <NA>   True   \n",
       "2  32416 2023-06-19 06:44:57.865882+00:00           <NA>   True   \n",
       "3  32418 2023-06-19 06:44:57.867936+00:00           <NA>   True   \n",
       "4  32419 2023-06-19 06:44:57.869497+00:00           <NA>   True   \n",
       "\n",
       "    pt_supervoxel_id          pt_root_id               pt_position  \n",
       "0  79871411195357919  720575940620919646  [538210, 198330, 105601]  \n",
       "1  77195887289571552  720575940611775973  [381459, 106763, 171723]  \n",
       "2  78955586729523534  720575940618135198  [486480, 133840, 123110]  \n",
       "3  79589661206805006  720575940624783287   [523906, 181893, 72760]  \n",
       "4  79589661207074646  720575940630755276   [523523, 181919, 80829]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1: query_table\n",
    "if species == \"mouse\":\n",
    "    proofread_table = \"proofreading_status_and_strategy\"\n",
    "elif species == \"human\":\n",
    "    proofread_table = \"proofreading_status_test\"\n",
    "elif species == \"fly\":\n",
    "    proofread_table = \"proofread_neurons\"\n",
    "elif species == \"zebrafish\":\n",
    "    proofread_table = \"somas\"\n",
    "\n",
    "print(f\"Querying table: {proofread_table}\")\n",
    "table = client.materialize.query_table(proofread_table)\n",
    "\n",
    "print(f\"Total records: {len(table):,}\")\n",
    "print(f\"\\nColumns: {list(table.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what columns contain neuron IDs\n",
    "print(\"Checking pt_root_id vs valid_id...\\n\")\n",
    "\n",
    "if 'pt_root_id' in table.columns:\n",
    "    pt_root_ids = np.unique(table['pt_root_id'].values)\n",
    "    print(f\"Unique pt_root_id values: {len(pt_root_ids):,}\")\n",
    "\n",
    "if 'valid_id' in table.columns:\n",
    "    valid_ids = np.unique(table['valid_id'].values)\n",
    "    print(f\"Unique valid_id values: {len(valid_ids):,}\")\n",
    "\n",
    "# Are they the same?\n",
    "if 'pt_root_id' in table.columns and 'valid_id' in table.columns:\n",
    "    same_count = (table['pt_root_id'] == table['valid_id']).sum()\n",
    "    print(f\"\\nRows where pt_root_id == valid_id: {same_count} / {len(table)}\")\n",
    "    \n",
    "    if same_count < len(table):\n",
    "        print(\"\\nThey're DIFFERENT! Let's see examples:\")\n",
    "        different = table[table['pt_root_id'] != table['valid_id']].head()\n",
    "        print(different[['pt_root_id', 'valid_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: live_query (Jeff's approach)\n",
    "live_result = client.materialize.live_query(\n",
    "    proofread_table,\n",
    "    datetime.now()\n",
    ")\n",
    "\n",
    "# Get unique pt_root_ids (this is what Jeff uses)\n",
    "neuron_ids = np.unique(np.asarray(live_result[\"pt_root_id\"]))\n",
    "\n",
    "print(f\"Using live_query + pt_root_id:\")\n",
    "print(f\"  Unique neurons: {len(neuron_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get Edit History for a Sample Neuron\n",
    "\n",
    "Let's look at the edit history structure for a single neuron to understand what data we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0, len(neuron_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample neuron\n",
    "idx = np.random.randint(0, len(neuron_ids))\n",
    "sample_neuron_id = neuron_ids[idx]\n",
    "\n",
    "print(f\"Getting edit history for neuron: {sample_neuron_id}\")\n",
    "print(f\"Using client.chunkedgraph.get_tabular_change_log()...\\n\")\n",
    "\n",
    "# Get edit history\n",
    "edit_history = client.chunkedgraph.get_tabular_change_log(\n",
    "    sample_neuron_id,\n",
    "    filtered=True\n",
    ")\n",
    "\n",
    "# If it's a dict, convert to DataFrame (depending on caveclient version)\n",
    "if isinstance(edit_history, dict):\n",
    "    # If there's only one top-level key, extract its value (should be a DataFrame)\n",
    "    if len(edit_history) == 1 and isinstance(list(edit_history.values())[0], pd.DataFrame):\n",
    "        edit_history = list(edit_history.values())[0]\n",
    "    else:\n",
    "        # Otherwise, create a DataFrame from the dict (rare)\n",
    "        edit_history = pd.DataFrame([edit_history])\n",
    "\n",
    "print(f\"Type: {type(edit_history)}\")\n",
    "print(f\"Number of edits: {len(edit_history)}\")\n",
    "print(f\"\\nColumns: {list(edit_history.columns)}\")\n",
    "\n",
    "if len(edit_history) == 0:\n",
    "    print(\"No edit history found for this neuron.\")\n",
    "else:\n",
    "    print(\"\\nFirst few edit rows:\")\n",
    "    display(edit_history.head())\n",
    "    print(\"\\nThe returned edit history has the following columns:\")\n",
    "    for col in edit_history.columns:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "    # Show a concise summary to help visualize the structure\n",
    "    print(\"\\nExample edit operation (all columns):\")\n",
    "    display(edit_history.iloc[0:1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Edit History Columns\n",
    "\n",
    "Key columns:\n",
    "- **`is_merge`**: True = merge operation (fixing split error), False = split operation (fixing merge error)\n",
    "- **`before_root_ids`**: Segment IDs before the edit\n",
    "- **`after_root_ids`**: Segment IDs after the edit  \n",
    "- **`timestamp`**: When the edit was made\n",
    "- **`operation_id`**: Unique ID for this edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze this neuron's edit types\n",
    "if 'is_merge' in edit_history.columns:\n",
    "    merge_count = (edit_history['is_merge'] == True).sum()\n",
    "    split_count = (edit_history['is_merge'] == False).sum()\n",
    "    \n",
    "    print(f\"Neuron {sample_neuron_id} edit breakdown:\")\n",
    "    print(f\"  Merge operations: {merge_count}\")\n",
    "    print(f\"  Split operations: {split_count}\")\n",
    "    print(f\"  Total: {len(edit_history)}\")\n",
    "else:\n",
    "    print(\"Note: 'is_merge' column not found. May need to infer from before/after root IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Multiple Neurons\n",
    "\n",
    "Now let's get edit histories for a sample of neurons to see the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample N random neurons\n",
    "np.random.seed(42)\n",
    "N = 100\n",
    "sample_size = min(N, len(neuron_ids))\n",
    "sampled_neurons = np.random.choice(neuron_ids, size=min(sample_size, len(neuron_ids)), replace=False)\n",
    "\n",
    "print(f\"Analyzing {len(sampled_neurons)} random neurons...\")\n",
    "print(f\"This will take ~1 minute...\\n\")\n",
    "\n",
    "def analyze_edit_history_tabular(edit_history):\n",
    "    \"\"\"\n",
    "    Minimal version of analyze_edit_history from analysis/analyze_edit_distributions.py\n",
    "    \"\"\"\n",
    "    # Handle None or empty\n",
    "    if edit_history is None or (hasattr(edit_history, \"__len__\") and len(edit_history) == 0):\n",
    "        return {\n",
    "            \"total_edits\": 0,\n",
    "            \"merge_edits\": 0,\n",
    "            \"split_edits\": 0,\n",
    "        }\n",
    "    # Handle dict (sometimes CAVEclient gives {root_id: DataFrame})\n",
    "    if isinstance(edit_history, dict):\n",
    "        if len(edit_history) == 0:\n",
    "            return {\n",
    "                \"total_edits\": 0,\n",
    "                \"merge_edits\": 0,\n",
    "                \"split_edits\": 0,\n",
    "            }\n",
    "        # Get first value (should be DataFrame)\n",
    "        edit_history = list(edit_history.values())[0]\n",
    "\n",
    "    if hasattr(edit_history, \"empty\") and edit_history.empty:\n",
    "        return {\n",
    "            \"total_edits\": 0,\n",
    "            \"merge_edits\": 0,\n",
    "            \"split_edits\": 0,\n",
    "        }\n",
    "\n",
    "    # Count merge/split edits\n",
    "    if 'is_merge' in edit_history.columns:\n",
    "        merge_edits = (edit_history['is_merge'] == True).sum()\n",
    "        split_edits = (edit_history['is_merge'] == False).sum()\n",
    "    else:\n",
    "        # Fallback: infer from before/after root IDs\n",
    "        merge_edits = 0\n",
    "        split_edits = 0\n",
    "        for _, row in edit_history.iterrows():\n",
    "            before_roots = row.get('before_root_ids', [])\n",
    "            after_roots = row.get('after_root_ids', [])\n",
    "            if isinstance(before_roots, str):\n",
    "                # Convert string repr to list for FlyWire dict\n",
    "                import ast\n",
    "                before_roots = ast.literal_eval(before_roots)\n",
    "            if isinstance(after_roots, str):\n",
    "                import ast\n",
    "                after_roots = ast.literal_eval(after_roots)\n",
    "            if len(before_roots) > len(after_roots):\n",
    "                merge_edits += 1\n",
    "            else:\n",
    "                split_edits += 1\n",
    "\n",
    "    total_edits = len(edit_history)\n",
    "    return {\n",
    "        \"total_edits\": int(total_edits),\n",
    "        \"merge_edits\": int(merge_edits),\n",
    "        \"split_edits\": int(split_edits),\n",
    "    }\n",
    "\n",
    "# Collect statistics\n",
    "results = []\n",
    "for i, nid in enumerate(sampled_neurons):\n",
    "    try:\n",
    "        edit_hist = client.chunkedgraph.get_tabular_change_log(nid, filtered=True)\n",
    "        stats = analyze_edit_history_tabular(edit_hist)\n",
    "        stats['neuron_id'] = nid\n",
    "        results.append(stats)\n",
    "        if (i + 1) % (sample_size // 10) == 0:\n",
    "            print(f\"  Processed {i+1}/{len(sampled_neurons)}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error with neuron {nid}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\n✓ Successfully analyzed {len(results_df)} neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Edit Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Edit Statistics:\")\n",
    "print(f\"  Mean edits per neuron: {results_df['total_edits'].mean():.2f}\")\n",
    "print(f\"  Median edits per neuron: {results_df['total_edits'].median():.0f}\")\n",
    "print(f\"  Max edits: {results_df['total_edits'].max()}\")\n",
    "print(f\"  Min edits: {results_df['total_edits'].min()}\")\n",
    "print(f\"\\nTotal across sample:\")\n",
    "print(f\"  Total edits: {results_df['total_edits'].sum()}\")\n",
    "print(f\"  Merge operations: {results_df['merge_edits'].sum()}\")\n",
    "print(f\"  Split operations: {results_df['split_edits'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If seaborn is not installed, you may need to run: !pip install seaborn\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Edit distribution with histogram and KDE fit\n",
    "sns.histplot(\n",
    "    results_df['total_edits'],\n",
    "    bins=20,\n",
    "    kde=True,\n",
    "    color='steelblue',\n",
    "    ax=axes[0],\n",
    "    edgecolor='black',\n",
    "    alpha=0.7\n",
    ")\n",
    "axes[0].axvline(results_df['total_edits'].mean(), color='red', linestyle='--',\n",
    "                label=f'Mean: {results_df[\"total_edits\"].mean():.1f}')\n",
    "axes[0].axvline(results_df['total_edits'].median(), color='green', linestyle='--',\n",
    "                label=f'Median: {results_df[\"total_edits\"].median():.0f}')\n",
    "axes[0].set_xlabel('Number of Edits per Neuron')\n",
    "axes[0].set_ylabel('Number of Neurons')\n",
    "axes[0].set_title('Distribution of Edits per Neuron')\n",
    "axes[0].legend()\n",
    "\n",
    "# Boxplot for Merge vs Split operations (per neuron)\n",
    "merge_counts = results_df['merge_edits']\n",
    "split_counts = results_df['split_edits']\n",
    "\n",
    "# Compute statistics\n",
    "def get_stats(arr):\n",
    "    arr = np.array(arr)\n",
    "    mean = np.mean(arr)\n",
    "    median = np.median(arr)\n",
    "    sem = stats.sem(arr)\n",
    "    ci95 = stats.t.interval(0.95, len(arr)-1, loc=mean, scale=sem) if len(arr) > 1 else (mean, mean)\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"median\": median,\n",
    "        \"ci95_lower\": ci95[0],\n",
    "        \"ci95_upper\": ci95[1],\n",
    "        \"sem\": sem,\n",
    "    }\n",
    "\n",
    "merge_stats = get_stats(merge_counts)\n",
    "split_stats = get_stats(split_counts)\n",
    "\n",
    "sns.boxplot(\n",
    "    data=[merge_counts, split_counts],\n",
    "    ax=axes[1],\n",
    "    palette=['coral', 'skyblue']\n",
    ")\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_xticklabels(['Merge', 'Split'])\n",
    "axes[1].set_ylabel('Edits per Neuron')\n",
    "axes[1].set_title('Merge vs Split Operations (Per Neuron)')\n",
    "\n",
    "# Display stats on the plot\n",
    "def annotate_stats(ax, stats, xpos, color):\n",
    "    y = stats['mean']\n",
    "    txt = (\n",
    "        f\"Mean: {stats['mean']:.2f}\\n\"\n",
    "        f\"Median: {stats['median']:.0f}\\n\"\n",
    "        f\"95% CI: [{stats['ci95_lower']:.2f}, {stats['ci95_upper']:.2f}]\"\n",
    "    )\n",
    "    ax.text(\n",
    "        xpos, y, txt,\n",
    "        color=color, fontsize=10,\n",
    "        va='bottom', ha='center',\n",
    "        bbox=dict(facecolor='white', alpha=0.8, edgecolor=color)\n",
    "    )\n",
    "    # Mark mean with a point\n",
    "    ax.plot(xpos, y, 'o', color=color)\n",
    "\n",
    "# Annotate Merge statistics\n",
    "annotate_stats(axes[1], merge_stats, xpos=0, color='coral')\n",
    "# Annotate Split statistics\n",
    "annotate_stats(axes[1], split_stats, xpos=1, color='skyblue')\n",
    "\n",
    "# Add suptitle with species, sample size, and full dataset size\n",
    "try:\n",
    "    suptitle_str = (\n",
    "        f\"Edit Distribution Analysis for {species.capitalize()} | \"\n",
    "        f\"Sample: {len(results_df):,} / All: {len(neuron_ids):,} neurons\"\n",
    "    )\n",
    "except Exception:\n",
    "    suptitle_str = \"Edit Distribution Analysis\"\n",
    "\n",
    "fig.suptitle(suptitle_str, fontsize=16, y=1.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Extrapolate to Full Dataset\n",
    "\n",
    "Based on this sample, estimate costs for the full proofread dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolate\n",
    "sample_size = len(results_df)\n",
    "full_dataset_size = len(neuron_ids)\n",
    "scaling_factor = full_dataset_size / sample_size\n",
    "\n",
    "estimated_total_edits = int(results_df['total_edits'].sum() * scaling_factor)\n",
    "estimated_merge_edits = int(results_df['merge_edits'].sum() * scaling_factor)\n",
    "estimated_split_edits = int(results_df['split_edits'].sum() * scaling_factor)\n",
    "\n",
    "print(f\"Extrapolation to Full Dataset ({full_dataset_size:,} neurons):\")\n",
    "print(f\"  Sample: {sample_size} neurons\")\n",
    "print(f\"  Scaling factor: {scaling_factor:.1f}x\")\n",
    "print(f\"\\nEstimated totals:\")\n",
    "print(f\"  Total edits: {estimated_total_edits:,}\")\n",
    "print(f\"  Merge operations: {estimated_merge_edits:,}\")\n",
    "print(f\"  Split operations: {estimated_split_edits:,}\")\n",
    "print(f\"\\nAverage per neuron: {results_df['total_edits'].mean():.2f} edits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To calculate GPU costs:\n",
    "1. Multiply edit counts by computational cost per edit type\n",
    "2. Cost per edit depends on:\n",
    "   - Model: Qwen 32B on 2x H100 GPUs\n",
    "   - Inference time per edit\n",
    "   - GPU utilization\n",
    "\n",
    "Formula:\n",
    "```\n",
    "Total Cost = (merge_edits × cost_per_merge) + (split_edits × cost_per_split)\n",
    "```\n",
    "\n",
    "This data provides the edit counts needed for that calculation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
